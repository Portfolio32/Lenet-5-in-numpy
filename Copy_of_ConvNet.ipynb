{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIo-TpA8J2Wf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcff4113-343b-4a55-e74b-e02475e29ca0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "from keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "(train_X, train_y), (test_X, test_y) = mnist.load_data()\n",
        "def zero_pad(X, pad):\n",
        "  return np.pad(X, ((0,0), (pad,pad),(pad,pad)))\n",
        "#Notes\n",
        "# Add activation function??? Will need to consider this in the backward pass\n",
        "train_X = zero_pad(train_X, 2)\n",
        "test_X = zero_pad((test_X), 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jp-0WHb0nlHh"
      },
      "outputs": [],
      "source": [
        "train_X = train_X / 255\n",
        "test_X = test_X / 255\n",
        "\n",
        "test_X = test_X\n",
        "test_y = test_y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cNwPxJwlD0gK"
      },
      "outputs": [],
      "source": [
        "W1 = np.random.randn(5,5,6)\n",
        "\n",
        "b1 = np.random.randn(1,1,6)\n",
        "\n",
        "hparameters1 = {'pad': 0,\n",
        "               \"stride\": 1}\n",
        "hparameters2 = {'stride':2, 'f':2}\n",
        "W2 = np.random.randn(5,5,6,16)\n",
        "b2 = np.random.rand(1,1,1,16)\n",
        "hparameters3 = {'pad': 0, 'stride': 1}\n",
        "hparameters4 = {'stride': 2, 'f':2}\n",
        "W3 = np.random.randn(5,5,16,120)\n",
        "b3 = np.random.randn(1,1,1,120)\n",
        "hparameters5 = {'pad': 0, 'stride': 1}\n",
        "W4 = np.random.randn(84,120)\n",
        "b4 = np.random.randn(84 , 1)\n",
        "W5 = np.random.randn(10, 84)\n",
        "b5 = np.random.randn(10, 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p1R7dM8hhPHv"
      },
      "outputs": [],
      "source": [
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2yNoT7QxCjYN",
        "outputId": "2615805b-950e-402f-c122-fa856557deeb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10000"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iee0x0aQi7hy",
        "outputId": "40a5bfe5-50f1-42d3-9f9e-5457afd5d2fa"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "<ipython-input-3-e64eb903140f>:82: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  Z[i, h, w, c] = tanh(conv_single_step(a_slice_prev, weights, biases))\n",
            "<ipython-input-3-e64eb903140f>:138: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
            "  Z[i, h, w, c] = tanh(conv_single_step(a_slice_prev, weights, biases))\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[2 2 2 ... 8 2 3]\n",
            "Accuracy: 0.0\n"
          ]
        }
      ],
      "source": [
        "for i in range(10):\n",
        "# First layer - Conv\n",
        "  A_prev = train_X\n",
        "  Y = train_y\n",
        "  Z1, cache_conv1 = conv_forward2D(A_prev, W1, b1, hparameters1)\n",
        "\n",
        "\n",
        "  # Second layer - Pool\n",
        "  P1, cache_pool1 = pool_forward(Z1, hparameters2, mode=\"max\")\n",
        "\n",
        "  # Third layer - Conv\n",
        "  Z2, cache_conv2 = conv_forward3D(P1, W2, b2, hparameters3)\n",
        "\n",
        "  # Fourth layer - Pool\n",
        "  P2, cache_pool2 = pool_forward(Z2, hparameters4, mode='max')\n",
        "\n",
        "\n",
        "  # Fifth layer - Conv\n",
        "  Z3, cache_conv3 = conv_forward3D(P2, W3, b3, hparameters5)\n",
        "\n",
        "\n",
        "  # Flatten and Fully Connected Layers\n",
        "  X = np.squeeze(Z3).T\n",
        "\n",
        "\n",
        "  Z4 = W4.dot(X) + b4\n",
        "\n",
        "\n",
        "  A1 = tanh(Z4)\n",
        "\n",
        "  Z5 = W5.dot(A1) + b5\n",
        "\n",
        "\n",
        "  A2 = softmax(Z5)\n",
        "\n",
        "\n",
        "  #Fully connceted backprop\n",
        "  dW4, db4, dW5, db5, dZ4 = backward_prop(Z4, A1, Z5, A2, W4, W5, X, Y)\n",
        "\n",
        "  #Conv/pool backprops\n",
        "  dZ3, dW3, db3 = conv_backward(dZ4.T,cache_conv3)\n",
        "  dP2 = pool_backward(dZ3,cache_pool2)\n",
        "\n",
        "  dZ2, dW2, db2 = conv_backward3D(dP2,cache_conv2)\n",
        "\n",
        "  dP1 = pool_backward(dZ2,cache_pool1)\n",
        "\n",
        "  dZ1, dW1,db1 = conv_backward2D(dP1,cache_conv1)\n",
        "\n",
        "  lr = 0.1\n",
        "  W1 = W1 - lr * dW1\n",
        "  b1 = b1 - lr * db1\n",
        "  W2 = W2 - lr * dW2\n",
        "  b2 = b2 - lr * db2\n",
        "  W3 = W3 - lr * dW3\n",
        "  b3 = b3 - lr * db3\n",
        "  W4 = W4 - lr * dW4\n",
        "  b4 = b4 - lr * db4\n",
        "  W5 = W5 - lr * dW5\n",
        "  b5 = b5 - lr * db5\n",
        "\n",
        "\n",
        "\n",
        "  # Step 1: Obtain Predictions\n",
        "  predictions = forward_pass(test_X, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5)\n",
        "\n",
        "  # Step 2: Convert predictions to class labels\n",
        "  predicted_labels = np.argmax(predictions, 0)\n",
        "  print(predicted_labels)\n",
        "  # Step 3: Compute Accuracy\n",
        "  Y1 = Y[:10000]\n",
        "  accuracy = np.sum(predictions == Y1) / Y1.size\n",
        "\n",
        "  print(\"Accuracy:\", accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fxauO21YNstN"
      },
      "outputs": [],
      "source": [
        "# Forward pass through the network\n",
        "def one_hot(Y):\n",
        "    x = int(Y.size)\n",
        "    y = int(Y.max())\n",
        "    one_hot_Y = np.zeros((x, y + 1))\n",
        "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "    one_hot_Y = one_hot_Y.T\n",
        "    return np.asarray(one_hot_Y)\n",
        "def forward_pass(X, W1, b1, W2, b2, W3, b3, W4, b4, W5, b5):\n",
        "    # First layer - Conv\n",
        "    Z1, cache_conv1 = conv_forward2D(X, W1, b1, hparameters1)\n",
        "    # Second layer - Pool\n",
        "    P1, cache_pool1 = pool_forward(Z1, hparameters2, mode=\"max\")\n",
        "    # Third layer - Conv\n",
        "    Z2, cache_conv2 = conv_forward3D(P1, W2, b2, hparameters3)\n",
        "    # Fourth layer - Pool\n",
        "    P2, cache_pool2 = pool_forward(Z2, hparameters4, mode='max')\n",
        "    # Fifth layer - Conv\n",
        "    Z3, cache_conv3 = conv_forward3D(P2, W3, b3, hparameters5)\n",
        "    # Flatten the output for fully connected layers\n",
        "    X_flatten = np.squeeze(Z3).T\n",
        "    # Sixth layer - FCL\n",
        "    Z4 = W4.dot(X_flatten) + b4\n",
        "    A1 = tanh(Z4)\n",
        "    # Seventh layer\n",
        "    Z5 = W5.dot(A1) + b5\n",
        "    A2 = softmax(Z5)\n",
        "    return A2\n",
        "\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
        "    m = Y.size\n",
        "    one_hot_Y = one_hot(Y)\n",
        "    dZ2 = A2 - one_hot_Y\n",
        "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
        "    db2 = 1 / m * np.sum(dZ2)\n",
        "    dZ1 = W2.T.dot(dZ2) * deriv_tanh(Z1)\n",
        "    dW1 = 1 / m * dZ1.dot(X.T)\n",
        "    db1 = 1 / m * np.sum(dZ1)\n",
        "    return dW1, db1, dW2, db2, dZ1\n",
        "\n",
        "\n",
        "def tanh(Z):\n",
        "  return np.tanh(Z)\n",
        "\n",
        "def deriv_tanh(Z):\n",
        "  return 1 - tanh(Z)*tanh(Z)\n",
        "\n",
        "def softmax(Z):\n",
        "    e_Z = np.exp(Z - np.max(Z))  # Normalize inputs to prevent overflow\n",
        "    return e_Z / np.sum(e_Z)\n",
        "\n",
        "\n",
        "def conv_single_step(a_slice_prev, W, b):\n",
        "  s = np.multiply(a_slice_prev, W)\n",
        "  Z = np.sum(s)\n",
        "  Z = Z + b\n",
        "  return Z\n",
        "\n",
        "def conv_forward2D(A_prev, W, b, hparameters):\n",
        "  #(m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "  (m, n_H_prev, n_W_prev) = A_prev.shape\n",
        "  #(f, f,n_C_prev,  n_C) = W.shape\n",
        "  (f, f,n_C) = W.shape\n",
        "  stride = hparameters['stride']\n",
        "  pad = hparameters['pad']\n",
        "  n_H = int((n_H_prev - f + 2*pad)/stride + 1)\n",
        "  n_W = int((n_W_prev - f + 2 * pad)/stride + 1)\n",
        "  # 512 - 7/2 + 1\n",
        "  Z = np.zeros((m, n_H, n_W, n_C))\n",
        "  for i in range(m):\n",
        "    a_prev_pad = A_prev[i]\n",
        "    for h in range(n_H):\n",
        "      vert_start = h *stride\n",
        "      vert_end = vert_start + f\n",
        "      for w in range(n_W):\n",
        "        horiz_start = w * stride\n",
        "        horiz_end = horiz_start + f\n",
        "        for c in range(n_C):\n",
        "          a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end]\n",
        "          weights = W[:,:,c]\n",
        "          biases = b[:,:,c]\n",
        "          Z[i, h, w, c] = tanh(conv_single_step(a_slice_prev, weights, biases))\n",
        "  cache = (A_prev, W, b, hparameters)\n",
        "\n",
        "  return Z, cache\n",
        "\n",
        "\n",
        "def pool_forward(A_prev, hparameters, mode = \"max\"):\n",
        "  (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "  f = hparameters['f']\n",
        "  stride = hparameters['stride']\n",
        "  n_H = int(1 + (n_H_prev - f) / stride)\n",
        "  n_W = int(1 + (n_W_prev - f) / stride)\n",
        "  n_C = n_C_prev\n",
        "  A = np.zeros((m, n_H, n_W, n_C))\n",
        "  for i in range(m):\n",
        "    a_prev_slice = A_prev[i]\n",
        "    for h in range(n_H):\n",
        "      vert_start = stride * h\n",
        "      vert_end = vert_start + f\n",
        "      for w in range(n_W):\n",
        "        horiz_start = stride * w\n",
        "        horiz_end = horiz_start + f\n",
        "        for c in range(n_C):\n",
        "          a_slice_prev = a_prev_slice[vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "          if mode == 'max':\n",
        "            A[i,h,w,c] = np.max(a_slice_prev)\n",
        "          elif mode == 'mean':\n",
        "            A[i,h,w,c] = np.mean(a_slice_prev)\n",
        "\n",
        "  cache = (A_prev, hparameters)\n",
        "\n",
        "  # Making sure your output shape is correct\n",
        "  assert(A.shape == (m, n_H, n_W, n_C))\n",
        "\n",
        "  return A, cache\n",
        "\n",
        "def conv_forward3D(A_prev, W, b, hparameters):\n",
        "  (m, n_H_prev, n_W_prev, n_C_prev) = A_prev.shape\n",
        "  (f, f,n_C_prev,  n_C) = W.shape\n",
        "  stride = hparameters['stride']\n",
        "  pad = hparameters['pad']\n",
        "  n_H = int((n_H_prev - f + 2*pad)/stride + 1)\n",
        "  n_W = int((n_W_prev - f + 2 * pad)/stride + 1)\n",
        "  Z = np.zeros((m, n_H, n_W, n_C))\n",
        "  for i in range(m):\n",
        "    a_prev_pad = A_prev[i]\n",
        "    for h in range(n_H):\n",
        "      vert_start = h *stride\n",
        "      vert_end = vert_start + f\n",
        "      for w in range(n_W):\n",
        "        horiz_start = w * stride\n",
        "        horiz_end = horiz_start + f\n",
        "        for c in range(n_C):\n",
        "          a_slice_prev = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end]\n",
        "          weights = W[:,:,:,c]\n",
        "          biases = b[:,:,:,c]\n",
        "          Z[i, h, w, c] = tanh(conv_single_step(a_slice_prev, weights, biases))\n",
        "\n",
        "  cache = (A_prev, W, b, hparameters)\n",
        "\n",
        "  return Z, cache\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def conv_backward3D(dZ, cache):\n",
        "\n",
        "  A_prev, W, b, hparameters = cache\n",
        "\n",
        "  m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "\n",
        "  f, f, n_C_prev, n_C = W.shape\n",
        "\n",
        "  stride = hparameters['stride']\n",
        "  pad = hparameters['pad']\n",
        "\n",
        "  m, n_H, n_W, n_C = dZ.shape\n",
        "\n",
        "  dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
        "  dW = np.zeros((f,f, n_C_prev, n_C))\n",
        "  db = np.zeros((1,1,1,n_C))\n",
        "\n",
        "  A_prev_pad = A_prev\n",
        "  dA_prev_pad = dA_prev\n",
        "\n",
        "  for i in range(m):\n",
        "      a_prev_pad = A_prev_pad[i]\n",
        "      da_prev_pad = dA_prev_pad[i]\n",
        "\n",
        "      for h in range(n_H):\n",
        "        for w in range(n_W):\n",
        "          for c in range(n_C):\n",
        "\n",
        "            vert_start = stride * h\n",
        "            vert_end = vert_start + f\n",
        "            horiz_start = stride * w\n",
        "            horiz_end = horiz_start + f\n",
        "            a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end]\n",
        "            da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += (W[:,:,:,c] * dZ[i, h, w, c]) * deriv_tanh(a_slice)\n",
        "            dW[:,:,:,c] += a_slice * dZ[i, h, w, c]\n",
        "            db[:,:,:,c] += dZ[i, h, w, c]\n",
        "\n",
        "      dA_prev[i, :, :, :] = da_prev_pad\n",
        "\n",
        "  assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
        "\n",
        "  return dA_prev, dW, db"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ku-kPm9IOidM"
      },
      "outputs": [],
      "source": [
        "def conv_backward2D(dZ, cache):\n",
        "  A_prev, W, b, hparameters = cache\n",
        "  m, n_H_prev, n_W_prev = A_prev.shape\n",
        "  f, f, n_C = W.shape\n",
        "  stride = hparameters['stride']\n",
        "  pad = hparameters['pad']\n",
        "  m, n_H, n_W, n_C = dZ.shape\n",
        "  dA_prev = np.zeros((m, n_H_prev, n_W_prev))\n",
        "  dW = np.zeros((f, f, n_C))\n",
        "  db = np.zeros((1,1,n_C))\n",
        "  A_prev_pad = zero_pad(A_prev, pad)\n",
        "  dA_prev_pad = zero_pad(dA_prev,pad)\n",
        "\n",
        "  for i in range(m):\n",
        "\n",
        "    a_prev_pad = A_prev_pad[i]\n",
        "    da_prev_pad = dA_prev_pad[i]\n",
        "    for h in range(n_H):\n",
        "      for w in range(n_W):\n",
        "        for c in range(n_C):\n",
        "          vert_start = stride * h\n",
        "          vert_end = vert_start + f\n",
        "          horiz_start = stride * w\n",
        "          horiz_end = horiz_start + f\n",
        "          a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end]\n",
        "\n",
        "          #I might need to get\n",
        "          da_prev_pad[vert_start:vert_end, horiz_start:horiz_end] += (W[:,:,c] * dZ[i, h, w, c]) * deriv_tanh(a_slice)\n",
        "          dW[:,:,c] += a_slice * dZ[i, h, w, c]\n",
        "          db[:,:,c] += dZ[i, h, w, c]\n",
        "\n",
        "    dA_prev[i, :, :] = da_prev_pad\n",
        "\n",
        "\n",
        "  assert(dA_prev.shape == (m, n_H_prev, n_W_prev))\n",
        "\n",
        "  return dA_prev, dW, db\n",
        "\n",
        "\n",
        "\n",
        "def conv_backward(dZ, cache):\n",
        "\n",
        "  A_prev, W, b, hparameters = cache\n",
        "\n",
        "  m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "\n",
        "  f, f, n_C_prev, n_C = W.shape\n",
        "\n",
        "  stride = hparameters['stride']\n",
        "  pad = hparameters['pad']\n",
        "\n",
        "  m, n_C = dZ.shape\n",
        "  n_H = 1\n",
        "  n_W = 1\n",
        "\n",
        "  dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
        "  dW = np.zeros((f,f, n_C_prev, 120))\n",
        "  db = np.zeros((1,1,1,120))\n",
        "\n",
        "  A_prev_pad = A_prev\n",
        "  dA_prev_pad = dA_prev\n",
        "\n",
        "  for i in range(m):\n",
        "      a_prev_pad = A_prev_pad[i]\n",
        "      da_prev_pad = dA_prev_pad[i]\n",
        "\n",
        "      for h in range(n_H):\n",
        "        for w in range(n_W):\n",
        "          for c in range(n_C):\n",
        "\n",
        "            vert_start = stride * h\n",
        "            vert_end = vert_start + f\n",
        "            horiz_start = stride * w\n",
        "            horiz_end = horiz_start + f\n",
        "            a_slice = a_prev_pad[vert_start:vert_end, horiz_start:horiz_end]\n",
        "            da_prev_pad[vert_start:vert_end, horiz_start:horiz_end, :] += W[:,:,:,c] * dZ[i, c]\n",
        "            dW[:,:,:,c] += a_slice * dZ[i, c]\n",
        "            db[:,:,:,c] += dZ[i, c]\n",
        "\n",
        "      dA_prev[i, :, :, :] = da_prev_pad\n",
        "\n",
        "  assert(dA_prev.shape == (m, n_H_prev, n_W_prev, n_C_prev))\n",
        "\n",
        "  return dA_prev, dW, db\n",
        "def create_mask_from_window(x):\n",
        "    mask = (x == np.max(x))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
        "    m = Y.size\n",
        "    one_hot_Y = one_hot(Y)\n",
        "    dZ2 = A2 - one_hot_Y\n",
        "    dW2 = 1 / m * dZ2.dot(A1.T)\n",
        "    db2 = 1 / m * np.sum(dZ2)\n",
        "    dZ1 = W2.T.dot(dZ2) * deriv_tanh(Z1)\n",
        "    dW1 = 1 / m * dZ1.dot(X.T)\n",
        "    db1 = 1 / m * np.sum(dZ1)\n",
        "    return dW1, db1, dW2, db2, dZ1\n",
        "\n",
        "\n",
        "\n",
        "def pool_backward(dA, cache, mode = \"max\"):\n",
        "    (A_prev, hparameters) = cache\n",
        "\n",
        "    stride = hparameters['stride']\n",
        "    f = hparameters['f']\n",
        "    m, n_H_prev, n_W_prev, n_C_prev = A_prev.shape\n",
        "    m, n_H, n_W, n_C = dA.shape\n",
        "    dA_prev = np.zeros((m, n_H_prev, n_W_prev, n_C_prev))\n",
        "    for i in range(m): # loop over the training examples\n",
        "        a_prev = A_prev[i]\n",
        "        for h in range(n_H):                   # loop on the vertical axis\n",
        "            for w in range(n_W):               # loop on the horizontal axis\n",
        "                for c in range(n_C):           # loop over the channels (depth)\n",
        "                    vert_start = stride * h\n",
        "                    vert_end = vert_start + f\n",
        "                    horiz_start = stride * w\n",
        "                    horiz_end = horiz_start + f\n",
        "                    if mode == \"max\":\n",
        "                        a_prev_slice = a_prev[vert_start:vert_end, horiz_start:horiz_end, c]\n",
        "                        mask = create_mask_from_window(a_prev_slice)\n",
        "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] += mask * dA[i,h,w,c]\n",
        "                    elif mode == \"average\":\n",
        "                        da = dA[i,h,w,c]\n",
        "                        shape = (f,f)\n",
        "                        dA_prev[i, vert_start: vert_end, horiz_start: horiz_end, c] +=  distribute_value(da, shape)\n",
        "    assert(dA_prev.shape == A_prev.shape)\n",
        "    return dA_prev\n",
        "\n",
        "\n",
        "def distribute_value(dz, shape):\n",
        "\n",
        "    (n_H, n_W) = shape\n",
        "    average = dz / (n_H * n_W)\n",
        "    a = np.ones(shape) * average\n",
        "\n",
        "    return a"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qJ31sjIdPaLj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}